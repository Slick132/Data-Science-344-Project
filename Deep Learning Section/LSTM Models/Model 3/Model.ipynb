{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Embedding, Conv1D, MaxPooling1D, Bidirectional,\n",
    "                                     LSTM, Dense, Dropout, BatchNormalization)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Randomly sample 100,000 observations from your data\n",
    "data_sample = data.sample(n=10000, random_state=42)\n",
    "\n",
    "# 2. Apply the preprocessing steps to this subset\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(data_sample['Lyrics_Processed'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "max_words = len(word_index) + 1\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(data_sample['Lyrics_Processed'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=30, truncating='post', padding='post')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(data_sample['genre'])\n",
    "labels_one_hot = to_categorical(labels_encoded, num_classes=6)\n",
    "labels = labels_one_hot\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "maxlen = 64\n",
    "embedding_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=30))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# First LSTM layer with dropout and kernel regularization\n",
    "model.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.4, kernel_regularizer=l2(0.05)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Intermediate LSTM layer with dropout and kernel regularization\n",
    "model.add(LSTM(1024, return_sequences=True, dropout=0.2, recurrent_dropout=0.4, kernel_regularizer=l2(0.05)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Second LSTM layer with dropout and kernel regularization\n",
    "model.add(LSTM(64, dropout=0.4, recurrent_dropout=0.2, kernel_regularizer=l2(0.05)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Dense layer with kernel regularization\n",
    "model.add(Dense(32, activation='tanh', kernel_regularizer=l2(0.05)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
